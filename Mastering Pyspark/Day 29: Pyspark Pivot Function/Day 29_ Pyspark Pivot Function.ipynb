{"cells":[{"cell_type":"markdown","source":["# Pivot in PySpark\n","The pivot operation in PySpark is used to transpose rows into columns based on a specified column's unique values. It's useful for creating wide-format data where values in one column become new column headers, and corresponding values from another column fill those headers.\n"],"metadata":{},"id":"3461ee6d"},{"cell_type":"markdown","source":["## Key Concepts\n","- **groupBy and pivot:** Used together to group data and pivot a column into new headers.\n","- **Aggregation Function:** Functions like `sum`, `avg`, `count` fill the values in pivoted columns.\n","- **Performance Considerations:** Pivoting can be expensive, especially with many unique values. Explicitly specifying pivot values improves performance.\n"],"metadata":{},"id":"2e26f00d"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"2a00e7fe-150c-4e1f-b14a-7f8388f3518a","normalized_state":"finished","queued_time":"2025-03-11T16:55:23.1224855Z","session_start_time":"2025-03-11T16:55:23.1238942Z","execution_start_time":"2025-03-11T16:55:36.4476885Z","execution_finish_time":"2025-03-11T16:55:36.997604Z","parent_msg_id":"bc544aeb-d948-459c-9b40-d0e7a9beed89"},"text/plain":"StatementMeta(, 2a00e7fe-150c-4e1f-b14a-7f8388f3518a, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64112eca"},{"cell_type":"code","source":["# Initialize Spark Session\n","spark = SparkSession.builder.appName('PivotExample').getOrCreate()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"2a00e7fe-150c-4e1f-b14a-7f8388f3518a","normalized_state":"finished","queued_time":"2025-03-11T16:55:23.123349Z","session_start_time":null,"execution_start_time":"2025-03-11T16:55:37.0003583Z","execution_finish_time":"2025-03-11T16:55:37.3759057Z","parent_msg_id":"33dd3da1-b43c-4440-ac49-b8de1ff77a8d"},"text/plain":"StatementMeta(, 2a00e7fe-150c-4e1f-b14a-7f8388f3518a, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59a7adf9"},{"cell_type":"code","source":["# Sample Data\n","data = [\n","    ('Product A', 'North', 100),\n","    ('Product A', 'South', 150),\n","    ('Product B', 'North', 200),\n","    ('Product B', 'South', 250),\n","    ('Product C', 'North', 300),\n","]\n","\n","columns = ['Product', 'Region', 'Sales']\n","df = spark.createDataFrame(data, columns)\n","df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"2a00e7fe-150c-4e1f-b14a-7f8388f3518a","normalized_state":"finished","queued_time":"2025-03-11T16:55:23.1240413Z","session_start_time":null,"execution_start_time":"2025-03-11T16:55:37.3784257Z","execution_finish_time":"2025-03-11T16:55:40.10028Z","parent_msg_id":"7b1f6ce4-f954-4088-bfb6-9b8ef52bb1a4"},"text/plain":"StatementMeta(, 2a00e7fe-150c-4e1f-b14a-7f8388f3518a, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+------+-----+\n|  Product|Region|Sales|\n+---------+------+-----+\n|Product A| North|  100|\n|Product A| South|  150|\n|Product B| North|  200|\n|Product B| South|  250|\n|Product C| North|  300|\n+---------+------+-----+\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"396f9ed3"},{"cell_type":"code","source":["# Pivot DataFrame\n","pivot_df = df.groupBy('Product').pivot('Region').agg(sum('Sales'))\n","pivot_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"2a00e7fe-150c-4e1f-b14a-7f8388f3518a","normalized_state":"finished","queued_time":"2025-03-11T16:55:23.1246453Z","session_start_time":null,"execution_start_time":"2025-03-11T16:55:40.1027769Z","execution_finish_time":"2025-03-11T16:55:43.5393105Z","parent_msg_id":"f83224e2-5802-4edf-80bf-e8eb83817287"},"text/plain":"StatementMeta(, 2a00e7fe-150c-4e1f-b14a-7f8388f3518a, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+-----+-----+\n|  Product|North|South|\n+---------+-----+-----+\n|Product A|  100|  150|\n|Product B|  200|  250|\n|Product C|  300| NULL|\n+---------+-----+-----+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d80f7f6e"},{"cell_type":"markdown","source":["### Explanation of Code\n","- **groupBy('Product')**: Groups data by the `Product` column.\n","- **pivot('Region')**: Converts unique `Region` values (North, South) into new columns.\n","- **agg(sum('Sales'))**: Computes the sum of `Sales` for each combination of `Product` and pivoted columns.\n"],"metadata":{},"id":"1b569afd"},{"cell_type":"markdown","source":["### Notes\n","- **Explicit Pivot Values:** To optimize performance, specify pivot values explicitly:\n","  ```python\n","  pivot_df = df.groupBy('Product').pivot('Region', ['North', 'South']).agg(sum('Sales'))\n","  ```\n","- **Handling Nulls:** If no data exists for a pivoted value, the resulting cell will be `null`.\n","- **Other Aggregations:** Functions like `avg`, `max`, and `min` can also be used.\n"],"metadata":{},"id":"40f0f086"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"nbformat":4,"nbformat_minor":5}