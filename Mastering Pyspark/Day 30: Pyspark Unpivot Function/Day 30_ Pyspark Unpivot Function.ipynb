{"cells":[{"cell_type":"markdown","source":["# Unpivot in PySpark\n","\n","The **unpivot** operation is used to transform wide-format data into a long-format table. This is useful when you need to restructure data for analysis."],"metadata":{},"id":"34bb3b60"},{"cell_type":"markdown","source":["## Sample Data\n","\n","We start with a DataFrame where sales data is stored in separate columns for different regions (North, South, East, West)."],"metadata":{},"id":"15a16720"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import expr\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName('UnpivotExample').getOrCreate()\n","\n","# Sample Data\n","data = [\n","    ('Product A', 100, 200, 150, 180),\n","    ('Product B', 90, 210, 160, 190),\n","    ('Product C', 120, 180, 140, 170)\n","]\n","\n","columns = ['Product', 'North', 'South', 'East', 'West']\n","df = spark.createDataFrame(data, columns)\n","\n","df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"7d9f7e7c-1fdd-45a1-b16b-de69a1805c7a","normalized_state":"finished","queued_time":"2025-03-12T16:44:22.7649142Z","session_start_time":"2025-03-12T16:44:22.7664803Z","execution_start_time":"2025-03-12T16:44:36.3504732Z","execution_finish_time":"2025-03-12T16:44:40.0637598Z","parent_msg_id":"c75eab36-34bc-4b7c-926c-a2903721c183"},"text/plain":"StatementMeta(, 7d9f7e7c-1fdd-45a1-b16b-de69a1805c7a, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+-----+-----+----+----+\n|  Product|North|South|East|West|\n+---------+-----+-----+----+----+\n|Product A|  100|  200| 150| 180|\n|Product B|   90|  210| 160| 190|\n|Product C|  120|  180| 140| 170|\n+---------+-----+-----+----+----+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d17c26ae"},{"cell_type":"markdown","source":["## Unpivoting Data\n","\n","We use the **stack()** function inside **selectExpr()** to transform the regional columns into rows. Each region name will now be stored in a single column (`Region`), and the corresponding sales values will be in another column (`Sales`)."],"metadata":{},"id":"ed490c2a"},{"cell_type":"code","source":["unpivot_expr = \"stack(4, 'North', North, 'South', South, 'East', East, 'West', West) as (Region, Sales)\"\n","df_unpivoted = df.selectExpr('Product', unpivot_expr)\n","\n","df_unpivoted.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"7d9f7e7c-1fdd-45a1-b16b-de69a1805c7a","normalized_state":"finished","queued_time":"2025-03-12T16:44:22.7884119Z","session_start_time":null,"execution_start_time":"2025-03-12T16:44:40.066523Z","execution_finish_time":"2025-03-12T16:44:41.2166651Z","parent_msg_id":"e1084a1c-5481-4941-808e-f28d181f3b38"},"text/plain":"StatementMeta(, 7d9f7e7c-1fdd-45a1-b16b-de69a1805c7a, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+------+-----+\n|  Product|Region|Sales|\n+---------+------+-----+\n|Product A| North|  100|\n|Product A| South|  200|\n|Product A|  East|  150|\n|Product A|  West|  180|\n|Product B| North|   90|\n|Product B| South|  210|\n|Product B|  East|  160|\n|Product B|  West|  190|\n|Product C| North|  120|\n|Product C| South|  180|\n|Product C|  East|  140|\n|Product C|  West|  170|\n+---------+------+-----+\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d822504"},{"cell_type":"markdown","source":["## Conclusion\n","\n","- **Unpivoting helps in data normalization**, making it easier to analyze and visualize.\n","- The `stack()` function is an efficient way to perform this transformation in PySpark.\n","- If the column names are dynamic, you can retrieve them using `df.columns` and construct the `selectExpr` dynamically."],"metadata":{},"id":"d9f7caa7"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"nbformat":4,"nbformat_minor":5}